# Multimodal RAG Pipeline for Document Analysis

This project outlines a complete, end-to-end pipeline for processing PDF documents to build a sophisticated Retrieval-Augmented Generation (RAG) system. The pipeline extracts text and figures from a document, generates AI-powered summaries for the figures, and builds a hybrid search retriever that can answer questions using both text and image context.

The workflow is broken down into **four main steps**, which should be run in order within Jupyter Notebook.

---

## üõ†Ô∏è Prerequisites

Before you begin, ensure you have the following:

### 1. Python Environment
A working Python environment (e.g., using Anaconda or `venv`) with **Jupyter Notebook** installed.

### 2. Required Libraries
Install all necessary libraries by running the following command in a notebook cell:

```bash
pip install boto3 python-dotenv pandas PyMuPDF langchain-openai langchain langchain-community chromadb rank_bm25
```

### 3. AWS Credentials
An AWS account with an S3 bucket already created.

### 4. .env File
Create a file named .env in the root of your project directory with the following content, replacing the placeholders with your actual credentials:

```env
AWS_ACCESS_KEY_ID=YOUR_AWS_ACCESS_KEY
AWS_SECRET_ACCESS_KEY=YOUR_AWS_SECRET_KEY
AWS_DEFAULT_REGION=your-aws-region
AWS_BUCKET_NAME=your-s3-bucket-name
OPENAI_API_KEY=your-openai-api-key
```

---

## üöÄ Workflow

### ‚úÖ Step 1: Run Textract Analysis and Get Raw Data

This first step handles the interaction with AWS Textract. It takes the source PDF, sends it to AWS for analysis, and saves the complete raw JSON response. This is a step that only needs to be run **once per document**.

**How to Run:**
- Ensure your `.env` file is correctly set up.
- Place the PDF file `test_info_extract.pdf` in the same directory.
- Run the corresponding cells in the Jupyter Notebook.

**Expected Output:**
- A new folder: `output/test_info_extract/`
- Containing file: `raw_textract_response.json`

---

### üß± Step 2: Parse JSON, Extract Layout, and Save Figures

This step parses the raw JSON response and extracts figure images from the original PDF.

**How to Run:**
- Run the corresponding cells in the Jupyter Notebook.

**Expected Output:**
- `layout.csv` inside `output/test_info_extract/`
- A folder `output/test_info_extract/figures/` containing images like:
  - `figure_1.png`, `figure_2.png`, etc.

---

### üñºÔ∏è Step 3: Integrate Figure Paths and Generate Summaries

This step:
- Takes `layout.csv` and the extracted figures,
- Uploads the figures to S3,
- Generates AI summaries for each figure,
- Saves the final merged file with summaries.

**How to Run:**
- Run the corresponding cells in the Jupyter Notebook.

**Expected Output:**
- Intermediate file: `layout_with_figures.csv`
- Final file: `layout_with_summaries.csv`  
  *(The 'Text' for each figure will be replaced by an AI-generated description.)*

---

### üîç Step 4: Build and Query the RAG Pipeline

This final step builds the `MultiVectorRetriever` with hybrid search and sets up the dynamic RAG chain.

**How to Run:**
- Run the final cells in the Jupyter Notebook.

**Expected Output:**
- The script will print:
  - Your query
  - A final, contextually-aware answer generated by the RAG pipeline