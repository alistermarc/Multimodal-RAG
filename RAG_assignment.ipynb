{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6752e984",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bb7675b",
   "metadata": {},
   "source": [
    "## Setting up of AWS Textract and AWS S3 for OCR\n",
    "\n",
    "Note: Put the access_key, scret_acess_key, default_region, bucket_name in the .env file\n",
    "There should already be a bucket in S3 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f7587",
   "metadata": {},
   "source": [
    "## Automated PDF Text Extraction with AWS Textract\n",
    "\n",
    "This Python script defines a class that automates the entire process of analyzing a PDF document using AWS Textract. It handles the complete cloud workflow by first uploading a local PDF file to an S3 bucket, then initiating an asynchronous analysis job with Textract to detect layout and tables. The script continuously polls AWS to check the job's status and, upon successful completion, retrieves all pages of the analysis results, saving the complete, raw data into a single JSON file on your local machine for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d848ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to S3: textract-analysis/test_info_extract.pdf\n",
      "Textract Job Started: c852bd39678d05af52b923989f4809995de2f1da5f93e167c7addc680e71967c\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Retrieved 1 pages of results.\n",
      "Saved raw Textract response to: output\\test_info_extract\\raw_textract_response.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output\\\\test_info_extract\\\\raw_textract_response.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class TextractJobRunner:\n",
    "    def __init__(self, base_dir=\"output\"):\n",
    "        self.textract = boto3.client(\n",
    "            'textract',\n",
    "            aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "            aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "            region_name=os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "        )\n",
    "        self.s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "            aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "            region_name=os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "        )\n",
    "        self.bucket_name = os.getenv(\"AWS_BUCKET_NAME\")\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def run_job_and_save_response(self, file_path):\n",
    "        \"\"\"\n",
    "        Takes a PDF, runs Textract analysis, and saves the raw JSON output.\n",
    "        \"\"\"\n",
    "        file_name_no_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_folder = os.path.join(self.base_dir, file_name_no_ext)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_key = f'textract-analysis/{os.path.basename(file_path)}'\n",
    "        self.s3.upload_file(file_path, self.bucket_name, s3_key)\n",
    "        print(f\"Uploaded to S3: {s3_key}\")\n",
    "\n",
    "        # Start Textract job\n",
    "        response = self.textract.start_document_analysis(\n",
    "            DocumentLocation={'S3Object': {'Bucket': self.bucket_name, 'Name': s3_key}},\n",
    "            FeatureTypes=['LAYOUT', 'TABLES']\n",
    "        )\n",
    "        job_id = response['JobId']\n",
    "        print(f\"Textract Job Started: {job_id}\")\n",
    "\n",
    "        # Poll for completion\n",
    "        while True:\n",
    "            result = self.textract.get_document_analysis(JobId=job_id)\n",
    "            status = result['JobStatus']\n",
    "            print(f\"Job status: {status}\")\n",
    "            if status in ['SUCCEEDED', 'FAILED']:\n",
    "                if status == 'FAILED':\n",
    "                    raise Exception(\"Textract job failed.\")\n",
    "                break\n",
    "            time.sleep(5)\n",
    "\n",
    "        # Retrieve all results using pagination\n",
    "        results = []\n",
    "        next_token = None\n",
    "        while True:\n",
    "            response = self.textract.get_document_analysis(JobId=job_id, NextToken=next_token) if next_token else self.textract.get_document_analysis(JobId=job_id)\n",
    "            results.append(response)\n",
    "            next_token = response.get('NextToken')\n",
    "            if not next_token:\n",
    "                break\n",
    "        print(f\"Retrieved {len(results)} pages of results.\")\n",
    "\n",
    "        # Save the raw results to a JSON file\n",
    "        json_path = os.path.join(output_folder, \"raw_textract_response.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Saved raw Textract response to: {json_path}\")\n",
    "\n",
    "        return json_path\n",
    "\n",
    "processor = TextractJobRunner()\n",
    "processor.run_job_and_save_response(\"test_info_extract.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd0db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1265db4d",
   "metadata": {},
   "source": [
    "## Textract JSON Parser and Figure Extractor\n",
    "\n",
    "This Python script defines two main functions to process the output from an AWS Textract analysis. The first function, save_layout, parses the raw JSON response to identify and organize all layout elements (like titles and text blocks) into a structured layout.csv file. The second function, save_figures, uses the same JSON data along with the original PDF to locate the coordinates of each figure, extracts the actual images by cropping the PDF, and saves them as individual PNG files into a specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ef81ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw layout to output\\test_info_extract\\layout.csv\n",
      "Saved 12 figures to 'output\\test_info_extract\\figures'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import fitz\n",
    "\n",
    "def save_layout(results, output_folder):\n",
    "    \"\"\"\n",
    "    This function is used to produce a csv from the raw json format.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    layout_counters = defaultdict(int)\n",
    "    reading_order = 0\n",
    "    line_map = {}\n",
    "    \n",
    "    for page in results:\n",
    "        line_map.update({b['Id']: b for b in page['Blocks'] if b['BlockType'] == 'LINE'})\n",
    "        layout_blocks = [b for b in page['Blocks'] if b['BlockType'].startswith('LAYOUT')]\n",
    "\n",
    "        for block in layout_blocks:\n",
    "            layout_key = block['BlockType'].replace('LAYOUT_', '').capitalize()\n",
    "            layout_counters[layout_key] += 1\n",
    "            layout_label = f\"{layout_key} {layout_counters[layout_key]}\"\n",
    "\n",
    "            line_text = ''\n",
    "            for rel in block.get('Relationships', []):\n",
    "                if rel.get('Type') == 'CHILD':\n",
    "                    line_text = ' '.join(line_map.get(i, {}).get('Text', '') for i in rel.get('Ids', []) if i in line_map)\n",
    "\n",
    "            rows.append({\n",
    "                'Page number': block.get('Page', 1),\n",
    "                'Layout': layout_label,\n",
    "                'Text': line_text.strip(),\n",
    "                'Reading Order': reading_order,\n",
    "                'Confidence score % (Layout)': block.get('Confidence', 0)\n",
    "            })\n",
    "            reading_order += 1\n",
    "\n",
    "    layout_path = os.path.join(output_folder, 'layout.csv')\n",
    "    pd.DataFrame(rows).to_csv(layout_path, index=False)\n",
    "    print(f\"Saved raw layout to {layout_path}\")\n",
    "\n",
    "def save_figures(results, pdf_path, output_folder):\n",
    "    \"\"\"\n",
    "    This function is used to retrieve the figures using the coordinates from raw json file and adds it to the layout.csv.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    all_blocks = [block for page in results for block in page.get('Blocks', [])]\n",
    "    figure_blocks = [b for b in all_blocks if b.get('BlockType') == 'LAYOUT_FIGURE']\n",
    "\n",
    "    for i, block in enumerate(figure_blocks, 1):\n",
    "        page_num = block.get('Page')\n",
    "        page = doc.load_page(page_num - 1)\n",
    "        box = block['Geometry']['BoundingBox']\n",
    "        \n",
    "        clip_rect = fitz.Rect(\n",
    "            box['Left'] * page.rect.width, box['Top'] * page.rect.height,\n",
    "            (box['Left'] + box['Width']) * page.rect.width,\n",
    "            (box['Top'] + box['Height']) * page.rect.height\n",
    "        )\n",
    "        \n",
    "        pix = page.get_pixmap(clip=clip_rect, dpi=200)\n",
    "        output_path = os.path.join(output_folder, f\"figure_{i}.png\")\n",
    "        pix.save(output_path)\n",
    "        \n",
    "    doc.close()\n",
    "    print(f\"Saved {len(figure_blocks)} figures to '{output_folder}'.\")\n",
    "\n",
    "\n",
    "base_dir = \"output\"\n",
    "document_folder = \"test_info_extract\"\n",
    "pdf_file_path = \"test_info_extract.pdf\"\n",
    "figure_output_folder = os.path.join(base_dir, document_folder, \"figures\")\n",
    "\n",
    "output_folder_path = os.path.join(base_dir, document_folder)\n",
    "json_file_path = os.path.join(output_folder_path, \"raw_textract_response.json\")\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    results_data = json.load(f)\n",
    "\n",
    "save_layout(results_data, output_folder_path)\n",
    "save_figures(results_data, pdf_file_path, figure_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf979f6",
   "metadata": {},
   "source": [
    "## CSV and Markdown Generation from Layout Data \n",
    "\n",
    "This Python script automates the final step of document reconstruction by taking a layout.csv file and a folder of previously extracted figure images as input. It first reads the CSV and systematically finds every row corresponding to a figure, updating its 'Text' column with the correct relative path to the saved image file. After saving this updated data to a new layout_with_figures.csv, the script then generates a complete final_layout.md file, converting titles to Markdown headers and the newly added figure paths into proper image links, effectively creating a readable version of the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513a10c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 12 figure image files.\n",
      "\n",
      "Successfully created new CSV with integrated figure filenames: output\\test_info_extract\\layout_with_figures.csv\n",
      "Successfully created Markdown file: output\\test_info_extract\\final_layout.md\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "csv_path = os.path.join(output_folder_path, 'layout.csv')\n",
    "figures_folder_path = os.path.join(output_folder_path, 'figures')\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_path, na_filter=False)\n",
    "\n",
    "figure_files = sorted(\n",
    "    [f for f in os.listdir(figures_folder_path) if f.startswith('figure_') and f.endswith('.png')],\n",
    "    key=lambda x: int(re.search(r'figure_(\\d+)\\.png', x).group(1))\n",
    ")\n",
    "print(f\"\\nFound {len(figure_files)} figure image files.\")\n",
    "\n",
    "figure_rows_indices = df[df['Layout'].str.startswith('Figure', na=False)].index\n",
    "\n",
    "# Loop through the figure rows and update the 'Text' column.\n",
    "for i, df_index in enumerate(figure_rows_indices):\n",
    "    figure_path = os.path.join('figures', figure_files[i])\n",
    "    df.loc[df_index, 'Text'] = figure_path\n",
    "\n",
    "output_csv_path = os.path.join(output_folder_path, 'layout_with_figures.csv')\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nSuccessfully created new CSV with integrated figure filenames: {output_csv_path}\")\n",
    "\n",
    "md_content = []\n",
    "for index, row in df.iterrows():\n",
    "    layout_type = str(row.get('Layout', '')).split(' ')[0]\n",
    "    text = str(row.get('Text', ''))\n",
    "\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    if layout_type == 'Title':\n",
    "        md_content.append(f\"# {text}\\n\")\n",
    "    elif layout_type == 'Header':\n",
    "        md_content.append(f\"## {text}\\n\")\n",
    "    elif layout_type == 'Figure':\n",
    "        md_content.append(f\"![{text}]({text})\\n\")\n",
    "    else: \n",
    "        md_content.append(f\"{text}\\n\")\n",
    "\n",
    "# Save the final markdown content to a file.\n",
    "md_path = os.path.join(output_folder_path, 'final_layout.md')\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(md_content))\n",
    "print(f\"Successfully created Markdown file: {md_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2655799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c30feb70",
   "metadata": {},
   "source": [
    "## Contextual Image Summarization with S3 Integration\n",
    "\n",
    "This Python script automates the process of generating rich, contextual descriptions for figures listed in a CSV file. It iterates through the layout data, and for each figure, it uploads the corresponding local image file to an AWS S3 bucket, generating a temporary, secure pre-signed URL. This URL, along with the text immediately before and after the figure, is then used to create a dynamic prompt for a large language model (gpt-4o-mini) via LangChain. After processing all figures in a batch, the script collects the AI-generated summaries and updates the original layout data, saving the final, enriched content into a new CSV file named layout_with_summaries.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff546797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded figure_1.png and generated pre-signed URL.\n",
      "Uploaded figure_2.png and generated pre-signed URL.\n",
      "Uploaded figure_3.png and generated pre-signed URL.\n",
      "Uploaded figure_4.png and generated pre-signed URL.\n",
      "Uploaded figure_5.png and generated pre-signed URL.\n",
      "Uploaded figure_6.png and generated pre-signed URL.\n",
      "Uploaded figure_7.png and generated pre-signed URL.\n",
      "Uploaded figure_8.png and generated pre-signed URL.\n",
      "Uploaded figure_9.png and generated pre-signed URL.\n",
      "Uploaded figure_10.png and generated pre-signed URL.\n",
      "Uploaded figure_11.png and generated pre-signed URL.\n",
      "Uploaded figure_12.png and generated pre-signed URL.\n",
      "File: figure_1.png\n",
      "Description: The image contains a bar plot comparing energy usage in kilowatt-hours (kWh) among three different categories of homes:\n",
      "\n",
      "1. **You**: Represented by a blue bar, it shows an energy usage of **125 kWh**.\n",
      "2. **Similar nearby homes**: Represented by an orange bar, it shows an energy usage of **103 kWh**.\n",
      "3. **Efficient nearby homes**: Represented by a green bar, it shows an energy usage of **49 kWh**.\n",
      "\n",
      "The blue bar (You) is the tallest, indicating that this particular home uses more energy than both the similar nearby homes and the efficient nearby homes. The orange bar (Similar nearby homes) is shorter, and the green bar (Efficient nearby homes) is the shortest, representing the least energy usage.\n",
      "\n",
      "File: figure_2.png\n",
      "Description: icon\n",
      "\n",
      "File: figure_3.png\n",
      "Description: icon\n",
      "\n",
      "File: figure_4.png\n",
      "Description: icon\n",
      "\n",
      "File: figure_5.png\n",
      "Description: The image features a cartoon-style depiction of a person standing next to a washing machine. The individual is holding a piece of fabric with a pattern, perhaps indicating laundry. The washing machine is simplistic in design, showing a circular opening where clothes would be loaded. The colors are bright and inviting, but there are no graphs or data visualizations present. Overall, it appears to emphasize the laundry context without providing specific energy-saving metrics or statistics.\n",
      "\n",
      "File: figure_6.png\n",
      "Description: icon\n",
      "\n",
      "File: figure_7.png\n",
      "Description: icon\n",
      "\n",
      "File: figure_8.png\n",
      "Description: icon\n",
      "\n",
      "File: figure_9.png\n",
      "Description: The image is an icon representing a smart thermostat setting. It features a circular dial with the number \"78\" prominently displayed in the center. The dial has markings around the perimeter, likely indicating temperature increments. The design is simple and uses a green color scheme, conveying a sense of energy efficiency. There are also small waves emanating from the top right, which may symbolize connectivity or communication features of the thermostat.\n",
      "\n",
      "File: figure_10.png\n",
      "Description: The image is a line graph comparing annual electricity usage in kilowatt-hours (kWh) across three categories: \"You,\" \"Similar Homes,\" and \"Efficient Homes.\" \n",
      "\n",
      "- **X-Axis**: Displays months from April to March.\n",
      "- **Y-Axis**: Indicates electricity usage, ranging from 0 to 200 kWh.\n",
      "- **Lines**:\n",
      "  - The **teal line** represents \"You\" and shows fluctuations with a peak around a certain month.\n",
      "  - The **orange line** signifies \"Similar Homes,\" which generally trends higher than the \"Efficient Homes\" line but shows variability.\n",
      "  - The **green line** stands for \"Efficient Homes,\" depicting consistently lower usage compared to both \"You\" and \"Similar Homes.\"\n",
      "\n",
      "Overall, the graph illustrates how individual electricity consumption compares to similar and more efficient homes throughout the year.\n",
      "\n",
      "File: figure_11.png\n",
      "Description: icon\n",
      "\n",
      "File: figure_12.png\n",
      "Description: Icon.\n",
      "\n",
      "Updated 'Figure 1' with new summary.\n",
      "Updated 'Figure 2' with new summary.\n",
      "Updated 'Figure 3' with new summary.\n",
      "Updated 'Figure 4' with new summary.\n",
      "Updated 'Figure 5' with new summary.\n",
      "Updated 'Figure 6' with new summary.\n",
      "Updated 'Figure 7' with new summary.\n",
      "Updated 'Figure 8' with new summary.\n",
      "Updated 'Figure 9' with new summary.\n",
      "Updated 'Figure 10' with new summary.\n",
      "Updated 'Figure 11' with new summary.\n",
      "Updated 'Figure 12' with new summary.\n",
      "\n",
      "Successfully created final CSV with summaries: output\\test_info_extract\\layout_with_summaries.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def upload_image_to_s3(local_path, bucket_name, s3_client):\n",
    "    \"\"\"\n",
    "    Uploads a local image file to an S3 bucket and returns a temporary,\n",
    "    secure pre-signed URL valid for one hour.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a unique object name for the image in the S3 bucket\n",
    "    s3_key = f\"figures/{os.path.basename(local_path)}\"\n",
    "    \n",
    "    # Upload the file\n",
    "    s3_client.upload_file(local_path, bucket_name, s3_key)\n",
    "    \n",
    "    # Generate a pre-signed URL that grants temporary access\n",
    "    url = s3_client.generate_presigned_url(\n",
    "        'get_object',\n",
    "        Params={'Bucket': bucket_name, 'Key': s3_key},\n",
    "        ExpiresIn=3600 \n",
    "    )\n",
    "    print(f\"Uploaded {os.path.basename(local_path)} and generated pre-signed URL.\")\n",
    "    return url\n",
    "\n",
    "csv_path = os.path.join(output_folder_path, 'layout_with_figures.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = os.getenv(\"AWS_BUCKET_NAME\")\n",
    "\n",
    "batch_input = []\n",
    "figure_filenames_in_order = []\n",
    "for i, row in df.iterrows():\n",
    "    if str(row.get('Layout')).startswith('Figure'):\n",
    "        image_path = os.path.join(output_folder_path, row['Text'])\n",
    "\n",
    "        # Get context from surrounding text rows\n",
    "        text_before = df.loc[i - 1, 'Text'] if i > 0 else \"No text before.\"\n",
    "        text_after = df.loc[i + 1, 'Text'] if i < len(df) - 1 else \"No text after.\"\n",
    "        \n",
    "        # Build the dynamic prompt for this specific figure\n",
    "        dynamic_prompt = f\"\"\"Describe the image in detail. It is part of a home energy report.\n",
    "\n",
    "        CONTEXT BEFORE IMAGE: \"{text_before}\"\n",
    "        CONTEXT AFTER IMAGE: \"{text_after}\"\n",
    "\n",
    "        Based on the context above, analyze the image. Be specific about graphs, such as bar plots. If it's a simple icon with no data, just say 'icon' - no other explanation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Upload the image to S3 and get the secure URL\n",
    "        image_s3_url = upload_image_to_s3(image_path, bucket_name, s3_client)\n",
    "        \n",
    "        if image_s3_url:\n",
    "            batch_input.append({\n",
    "                \"image_url_input\": image_s3_url,\n",
    "                \"prompt_text\": dynamic_prompt,\n",
    "                \"original_path\": os.path.basename(image_path)\n",
    "            })\n",
    "            figure_filenames_in_order.append(os.path.basename(image_path))\n",
    "\n",
    "# Define the LangChain Prompt Template and Chain \n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": \"{prompt_text}\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"{image_url_input}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Run the Batch Process\n",
    "if batch_input:\n",
    "\n",
    "    image_summaries = chain.batch(batch_input, {\"max_concurrency\": 5})\n",
    "    for i, summary in enumerate(image_summaries):\n",
    "        filename = figure_filenames_in_order[i]\n",
    "        print(f\"File: {filename}\")\n",
    "        print(f\"Description: {summary}\\n\")\n",
    "\n",
    "    figure_indices = df[df['Layout'].str.startswith('Figure', na=False)].index.tolist()\n",
    "\n",
    "    summary_counter = 0\n",
    "    for idx in figure_indices:\n",
    "        if summary_counter < len(image_summaries):\n",
    "            original_layout_label = df.loc[idx, 'Layout']\n",
    "            new_summary = image_summaries[summary_counter]\n",
    "            \n",
    "            df.loc[idx, 'Text'] = new_summary\n",
    "            print(f\"Updated '{original_layout_label}' with new summary.\")\n",
    "            \n",
    "            summary_counter += 1\n",
    "\n",
    "    output_dir = os.path.dirname(csv_path) or '.'\n",
    "    output_path = os.path.join(output_dir, 'layout_with_summaries.csv')\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSuccessfully created final CSV with summaries: {output_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"No figures found in the CSV to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca94609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570fb2a9",
   "metadata": {},
   "source": [
    "## Multimodal Data Preparation for Advanced RAG\n",
    "\n",
    "This Python script prepares a sophisticated data structure for a Retrieval-Augmented Generation (RAG) system by loading two separate CSV files: one containing the original document layout with figure paths, and another with AI-generated summaries. It processes this data by creating \"parent\" documents from the original content to serve as the ground truth for retrieval. It then applies a custom chunking logic to the AI-generated summaries, iteratively grouping them into larger, context-rich \"child\" documents up to a 1000-character limit. The final output consists of two aligned sets of documents (original parents and chunked summary children), perfectly structured for use in a MultiVectorRetriever where searches can be performed on the rich summaries to retrieve the original, precise content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01563ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and sorted data from both CSV files.\n",
      "Creating parent documents and custom child chunks...\n",
      "Created 6 child chunks from 38 parent documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Load environment variables from your .env file\n",
    "load_dotenv()\n",
    "\n",
    "csv_figures_path = 'output/test_info_extract/layout_with_figures.csv'\n",
    "csv_summaries_path = 'output/test_info_extract/layout_with_summaries.csv'\n",
    "\n",
    "try:\n",
    "    df_figures = pd.read_csv(csv_figures_path, na_filter=False)\n",
    "    df_summaries = pd.read_csv(csv_summaries_path, na_filter=False)\n",
    "    df_figures_sorted = df_figures.sort_values(by='Reading Order').reset_index(drop=True)\n",
    "    df_summaries_sorted = df_summaries.sort_values(by='Reading Order').reset_index(drop=True)\n",
    "    assert len(df_figures_sorted) == len(df_summaries_sorted), \"CSV files must have the same number of rows.\"\n",
    "    print(f\"Successfully loaded and sorted data from both CSV files.\")\n",
    "except (FileNotFoundError, KeyError, AssertionError) as e:\n",
    "    print(f\"Error loading or processing CSV files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Create Parent Documents and Custom Child Chunks\n",
    "print(\"Creating parent documents and custom child chunks...\")\n",
    "parent_documents = []\n",
    "child_docs = []\n",
    "parent_doc_ids = []\n",
    "\n",
    "for index, row in df_figures_sorted.iterrows():\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    parent_doc_ids.append(doc_id)\n",
    "    parent_content = f\"{row['Layout']}: {row['Text']}\"\n",
    "    parent_documents.append(Document(page_content=parent_content, metadata={\"doc_id\": doc_id}))\n",
    "\n",
    "max_chars = 1000\n",
    "current_chunk_text = \"\"\n",
    "ids_for_current_chunk = []\n",
    "texts_to_chunk = [f\"{row['Layout']}: {row['Text']}\" for index, row in df_summaries_sorted.iterrows()]\n",
    "\n",
    "for i, text_to_add in enumerate(texts_to_chunk):\n",
    "    doc_id = parent_doc_ids[i]\n",
    "    if not current_chunk_text:\n",
    "        current_chunk_text = text_to_add\n",
    "        ids_for_current_chunk.append(doc_id)\n",
    "        continue\n",
    "    if len(current_chunk_text) + len(text_to_add) + 1 > max_chars:\n",
    "        metadata = {\"parent_doc_ids\": \",\".join(ids_for_current_chunk)}\n",
    "        child_docs.append(Document(page_content=current_chunk_text, metadata=metadata))\n",
    "        current_chunk_text = text_to_add\n",
    "        ids_for_current_chunk = [doc_id]\n",
    "    else:\n",
    "        current_chunk_text += \"\\n\" + text_to_add\n",
    "        ids_for_current_chunk.append(doc_id)\n",
    "if current_chunk_text:\n",
    "    metadata = {\"parent_doc_ids\": \",\".join(ids_for_current_chunk)}\n",
    "    child_docs.append(Document(page_content=current_chunk_text, metadata=metadata))\n",
    "print(f\"Created {len(child_docs)} child chunks from {len(parent_documents)} parent documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c97aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a6c349",
   "metadata": {},
   "source": [
    "## Hybrid Search and Parent Document Retrieval\n",
    "This Python script sets up a sophisticated, two-part retrieval system for a RAG pipeline. First, it creates a hybrid search mechanism by combining a keyword-based retriever (BM25Retriever) with a semantic vector-based retriever (Chroma), allowing it to find documents based on both exact words and contextual meaning. This hybrid search is performed on a collection of smaller \"child\" documents (like summaries). The script then defines a custom function that takes the results of this hybrid search, extracts the IDs of the original \"parent\" documents they correspond to, and fetches those complete parent documents from an in-memory store, ensuring that the final output is the full, original source content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully built the vector store and retriever.\n"
     ]
    }
   ],
   "source": [
    "# Set up the Retriever\n",
    "emb = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(collection_name=\"rag_s3_ordered_retriever\", embedding_function=emb)\n",
    "vectorstore.add_documents(child_docs)\n",
    "store = InMemoryStore()\n",
    "store.mset(list(zip(parent_doc_ids, parent_documents)))\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(child_docs)\n",
    "bm25_retriever.k = 3\n",
    "semantic_retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, semantic_retriever], weights=[0.5, 0.5])\n",
    "\n",
    "def get_parents_from_hybrid_search(query):\n",
    "    child_chunks = hybrid_retriever.invoke(query)\n",
    "    \n",
    "    # Get the string of parent IDs from the metadata\n",
    "    ordered_parent_ids = []\n",
    "    seen_ids = set()\n",
    "    for chunk in child_chunks:\n",
    "        parent_ids_str = chunk.metadata.get(\"parent_doc_ids\", \"\")\n",
    "        for p_id in parent_ids_str.split(\",\"):\n",
    "            if p_id and p_id not in seen_ids:\n",
    "                ordered_parent_ids.append(p_id)\n",
    "                seen_ids.add(p_id)\n",
    "                \n",
    "    return store.mget(ordered_parent_ids)\n",
    "\n",
    "retriever = RunnableLambda(get_parents_from_hybrid_search)\n",
    "print(\"\\nSuccessfully built the vector store and retriever.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ace23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "828619be",
   "metadata": {},
   "source": [
    "## Dynamic Multimodal RAG Chain\n",
    "\n",
    "This Python script defines the final, dynamic stage of a Retrieval-Augmented Generation (RAG) pipeline designed to handle both text and images. It creates a chain that first retrieves relevant documents from your vector store. A custom parsing function then processes these documents: if a document represents a figure, it uploads the corresponding image to AWS S3 on-the-fly to generate a secure, temporary URL. Finally, another function dynamically assembles a multimodal prompt for the language model, combining all the retrieved text with any generated image URLs, allowing the AI to generate a comprehensive answer based on both the textual and visual context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45530fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG Chain with On-the-Fly URL Generation\n",
    "def parse_docs_and_generate_urls(docs):\n",
    "    image_urls, text_content = [], []\n",
    "    for doc in docs:\n",
    "        if not doc: continue\n",
    "        \n",
    "        page_content = doc.page_content\n",
    "        # Check if the document is a figure\n",
    "        if \"Figure\" in page_content.split(':')[0]:\n",
    "            # The figure path is in the text part of the content\n",
    "            relative_fig_path = page_content.split(': ', 1)[1]\n",
    "            local_fig_path = os.path.join(base_dir, relative_fig_path)\n",
    "            \n",
    "            url = upload_image_to_s3(local_fig_path, bucket_name, s3_client)\n",
    "            if url:\n",
    "                image_urls.append(url)\n",
    "                # Add the figure's layout label as text context\n",
    "                text_content.append(page_content.split(':')[0] + \":\")\n",
    "        else:\n",
    "            text_content.append(page_content)\n",
    "            \n",
    "    return {\"images\": image_urls, \"texts\": text_content}\n",
    "\n",
    "def build_prompt_with_urls(inputs):\n",
    "    context_text = \"\\n\".join(inputs[\"context\"]['texts']).strip()\n",
    "    prompt_content = [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"\"\"Answer the question based only on the following context.\n",
    "\n",
    "Context:\n",
    "---\n",
    "{context_text}\n",
    "---\n",
    "\n",
    "Question: {inputs['question']}\n",
    "\"\"\"\n",
    "    }]\n",
    "    for url in inputs[\"context\"][\"images\"]:\n",
    "        prompt_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": url}})\n",
    "    return [HumanMessage(content=prompt_content)]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(parse_docs_and_generate_urls), \"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(build_prompt_with_urls)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7516eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f09d40e",
   "metadata": {},
   "source": [
    "## RAG Pipeline Execution with Source Inspection\n",
    "\n",
    "This Python script demonstrates how to run and inspect the results of the complete Retrieval-Augmented Generation (RAG) pipeline. It defines a sample query and then invokes a special chain designed for debugging, which returns not only the final, AI-generated answer but also the specific \"reference chunks\" (the parent documents) that were retrieved from the vector store to create that answer. The script then prints both the retrieved source documents for verification and the final answer, providing a clear way to understand what information the language model used as its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Search for: 'What was the energy usage compared to neighbors?' ---\n",
      "\n",
      "Generating answer...\n",
      "Retrieving reference documents...\n",
      "\n",
      "--- REFERENCE CHUNKS (Parent Documents) ---\n",
      "Title 1: Home Energy Report: electricity\n",
      "------------------------------\n",
      "Text 1: March report Account number: 954137 Service address: 1627 Tulip Lane\n",
      "------------------------------\n",
      "Text 2: Dear JILL DOE, here is your usage analysis for March.\n",
      "------------------------------\n",
      "Text 3: Your electric use:\n",
      "------------------------------\n",
      "Text 4: Above typical use\n",
      "------------------------------\n",
      "Text 5: 18% more than similar nearby homes\n",
      "------------------------------\n",
      "Figure 1: figures\\figure_1.png\n",
      "------------------------------\n",
      "Figure 10: figures\\figure_10.png\n",
      "------------------------------\n",
      "Text 20: Save more this spring\n",
      "------------------------------\n",
      "Text 21: Reduce use and save money on your electric bill with these thorough tips, from the kitchen to the laundry room.\n",
      "------------------------------\n",
      "Figure 11: figures\\figure_11.png\n",
      "------------------------------\n",
      "Text 17: Older model refrigerators are very inefficient. You can make up the cost of a new Energy Star refrigerator in energy savings in just a few years.\n",
      "------------------------------\n",
      "Text 18: Adjust thermostat settings Biggest energy saving option\n",
      "------------------------------\n",
      "Figure 8: figures\\figure_8.png\n",
      "------------------------------\n",
      "Text 19: Set your smart thermostat to save more 78�� energy during high-cost hours. Pre-heat your home on cold days so that you can save more energy.\n",
      "------------------------------\n",
      "Figure 9: figures\\figure_9.png\n",
      "------------------------------\n",
      "\n",
      "--- FINAL ANSWER ---\n",
      "Your energy usage was 18% more than similar nearby homes. You used 125 kWh, while similar nearby homes used 103 kWh.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    query = \"What was the energy usage compared to neighbors?\"\n",
    "    \n",
    "    print(f\"\\n--- Example Search for: '{query}' ---\")\n",
    "\n",
    "    # 1. First, get the final answer from the main chain\n",
    "    print(\"\\nGenerating answer...\")\n",
    "    answer = chain.invoke(query)\n",
    "\n",
    "    # 2. Then, get the reference documents from the retriever separately to inspect them\n",
    "    print(\"Retrieving reference documents...\")\n",
    "    reference_docs = retriever.invoke(query)\n",
    "\n",
    "    # 3. Print the retrieved context (the reference parent documents)\n",
    "    print(\"\\n--- REFERENCE CHUNKS (Parent Documents) ---\")\n",
    "    if reference_docs:\n",
    "        for doc in reference_docs:\n",
    "            if doc:\n",
    "                print(doc.page_content)\n",
    "    else:\n",
    "        print(\"No context retrieved.\")\n",
    "\n",
    "    # 4. Print the final answer\n",
    "    print(\"\\n--- FINAL ANSWER ---\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9534f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21a9317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Search for: 'How much is my electrical consumption for April and how much is for the similar homes?' ---\n",
      "\n",
      "Generating answer...\n",
      "Retrieving reference documents...\n",
      "\n",
      "--- REFERENCE CHUNKS (Parent Documents) ---\n",
      "Title 1: Home Energy Report: electricity\n",
      "Text 1: March report Account number: 954137 Service address: 1627 Tulip Lane\n",
      "Text 2: Dear JILL DOE, here is your usage analysis for March.\n",
      "Text 3: Your electric use:\n",
      "Text 4: Above typical use\n",
      "Text 5: 18% more than similar nearby homes\n",
      "Figure 1: figures\\figure_1.png\n",
      "Figure 10: figures\\figure_10.png\n",
      "Text 20: Save more this spring\n",
      "Text 21: Reduce use and save money on your electric bill with these thorough tips, from the kitchen to the laundry room.\n",
      "Figure 11: figures\\figure_11.png\n",
      "Figure 5: figures\\figure_5.png\n",
      "Text 13: Watch this space for new ways to save energy each month.\n",
      "Footer 1: Turn over for more savings ideas.\n",
      "Figure 6: figures\\figure_6.png\n",
      "Title 2: Your top three tailored energy-saving tips\n",
      "Text 14: Caulk windows and doors Save money and energy\n",
      "Text 15: One of the biggest money-wasters in your home is drafty windows and doors. Caulking drafty areas is a simple DIY project that will pay off.\n",
      "Figure 7: figures\\figure_7.png\n",
      "Text 16: Upgrade your refrigerator Look for an Energy Star label\n",
      "\n",
      "--- FINAL ANSWER ---\n",
      "Your electrical consumption for April is 125 kWh, while the consumption for similar nearby homes is 103 kWh.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    query = \"How much is my electrical consumption for April and how much is for the similar homes?\"\n",
    "    \n",
    "    print(f\"\\n--- Example Search for: '{query}' ---\")\n",
    "\n",
    "    # 1. First, get the final answer from the main chain\n",
    "    print(\"\\nGenerating answer...\")\n",
    "    answer = chain.invoke(query)\n",
    "\n",
    "    # 2. Then, get the reference documents from the retriever separately to inspect them\n",
    "    print(\"Retrieving reference documents...\")\n",
    "    reference_docs = retriever.invoke(query)\n",
    "\n",
    "    # 3. Print the retrieved context (the reference parent documents)\n",
    "    print(\"\\n--- REFERENCE CHUNKS (Parent Documents) ---\")\n",
    "    if reference_docs:\n",
    "        for doc in reference_docs:\n",
    "            if doc:\n",
    "                print(doc.page_content)\n",
    "    else:\n",
    "        print(\"No context retrieved.\")\n",
    "\n",
    "    # 4. Print the final answer\n",
    "    print(\"\\n--- FINAL ANSWER ---\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ab7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
